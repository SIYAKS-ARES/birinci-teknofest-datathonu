# Stacked Sinir AÄŸÄ± (Stacked NN)

Bu Ã§Ã¶zÃ¼m, farklÄ± modellerin gÃ¼Ã§lÃ¼ yÃ¶nlerini bir araya getirerek oluÅŸturulmuÅŸ bir **stacked ensemble** yaklaÅŸÄ±mÄ±dÄ±r. Ã–zellikle sÄ±zÄ±ntÄ±sÄ±z (leak-free) veri iÅŸleme, OOF tahminlerin tekrar kullanÄ±mÄ± ve outlier sÄ±nÄ±flandÄ±rmasÄ± gibi ileri dÃ¼zey teknikleri barÄ±ndÄ±rmaktadÄ±r.

---

## ğŸš§ Stratejiye Genel BakÄ±ÅŸ

- FarklÄ± modeller oluÅŸturuldu ve bu modellerin **out-of-fold (OOF)** tahminleri daha sonra baÅŸka modeller iÃ§in giriÅŸ (feature) olarak kullanÄ±ldÄ±.
- Ensemble stratejisinde temel fikir: FarklÄ± yapÄ±daki modellerin hatalarÄ±nÄ±n birbiriyle Ã¶rtÃ¼ÅŸmemesi ve bu sayede genel hata oranÄ±nÄ±n dÃ¼ÅŸmesidir.
- Ä°ki farklÄ± final gÃ¶nderimi yapÄ±ldÄ±:
  - Ä°lki: Ridge ile birleÅŸtirilmiÅŸ ensemble
  - Ä°kincisi (birinciliÄŸi getiren): GeniÅŸletilmiÅŸ sinir aÄŸÄ± (NN) modeli

---

## ğŸ§ª Veri Ä°ÅŸleme ve Ã‡apraz DoÄŸrulama

- **20-fold Ã§apraz doÄŸrulama** kullanÄ±ldÄ±. Bu sayede modelin genellenebilirliÄŸi artÄ±rÄ±ldÄ±.
- Kategorik deÄŸiÅŸkenler iÃ§in **target encoding** uygulandÄ±.
  - Ortalama yerine **medyan** kullanÄ±ldÄ±.
  - Her fold'da **yeniden hesaplandÄ±**, bÃ¶ylece veri sÄ±zÄ±ntÄ±sÄ± (data leakage) Ã¶nlendi.

---

## ğŸ¯ Outlier (AykÄ±rÄ± DeÄŸer) SÄ±nÄ±flandÄ±rmasÄ±

BazÄ± modellerin fiyatlarÄ± doÄŸru tahmin edememesi outlier'lardan kaynaklanÄ±yordu. Bu nedenle Ã¶nce outlierâ€™lar sÄ±nÄ±flandÄ±rÄ±ldÄ± ve bu bilgi diÄŸer modellere eklendi.

```python
def bin_price(data):
    df = data.copy()
    Q1 = np.percentile(df['price'], 25)
    Q3 = np.percentile(df['price'], 75)
    IQR = Q3 - Q1
    upper_bound = Q3 + 1.5 * IQR
    df['price_bin'] = (df['price'] < upper_bound).astype(int)
    return df

	â€¢	price_bin Ã¶zelliÄŸi CatBoostClassifier ile tahmin edildi ve diÄŸer modellere Ã¶zellik olarak eklendi.

â¸»

ğŸ§  KullanÄ±lan Ana Modeller

1. CatBoostClassifier (Outlier sÄ±nÄ±flayÄ±cÄ±)

cat_params2 = {
    'early_stopping_rounds': 25,
    'use_best_model': True,
    "verbose": False,
    'cat_features': cat_cols,
    'min_data_in_leaf': 16, 
    'learning_rate': 0.0335, 
    'random_strength': 11.66, 
    'l2_leaf_reg': 17.70, 
    'max_depth': 10, 
    'subsample': 0.947, 
    'border_count': 130, 
    'bagging_temperature': 24.03
}

	â€¢	CatBoostClassifier tahminleri baÅŸka modellere girdi (feature) olarak verildi.

â¸»

2. LightGBM (LGBM5)

lgb_params = {
    'verbose': -1,
    'early_stopping_rounds': 25,
    'loss_function': "RMSE",
    'n_estimators': 2000, 
    'max_bin': 30000,
}

	â€¢	Kategorik deÄŸiÅŸkenler label encoding ile iÅŸlendi.
	â€¢	Nadir kategoriler â€œrareâ€ olarak gruplanarak overfitting azaltÄ±ldÄ±.

â¸»

3. SVR (Support Vector Regression)
	â€¢	rbf Ã§ekirdeÄŸi kullanÄ±ldÄ±.
	â€¢	Ã–zellikle orta-dÃ¼zey fiyatlar iÃ§in stabil performans verdi.
	â€¢	SVR OOF tahminleri sinir aÄŸÄ± modeline eklendi.

â¸»

4. AutoGluon (FastAI Modeli)

predictor = TabularPredictor(label='price',
                             eval_metric='rmse',
                             problem_type="regression").fit(
    X_train,
    pseudo_data=data_original, 
    num_bag_folds=10,
    num_bag_sets=2,
    time_limit=1800,
    included_model_types=['FASTAI'], 
    keep_only_best=True,
    presets="best_quality"
)

	â€¢	Nested CV uygulandÄ±.
	â€¢	Not: AutoGluonâ€™un fit() fonksiyonu orijinal veriyi kullanmaz, bu yÃ¼zden fit_pseudolabel() kullanÄ±lmasÄ± Ã¶nerilir.

â¸»

ğŸ”„ Ensemble YapÄ±sÄ±: GeniÅŸletilmiÅŸ Sinir AÄŸÄ±

Finalde kullanÄ±lan stacked NN, aÅŸaÄŸÄ±daki 4 modelin OOF tahminlerini Ã¶zellik olarak aldÄ±:
	1.	SVR tahminleri
	2.	CatBoostClassifier tahminleri
	3.	LGBM5 tahminleri
	4.	XGB tahminleri (kaynaÄŸÄ± unutulmuÅŸ)

	â€¢	Bu yapÄ±, klasik ensemble yaklaÅŸÄ±mÄ±ndan daha iyi sonuÃ§ verdi.
	â€¢	NN modeli deÄŸiÅŸikliklere karÅŸÄ± robust olduÄŸu iÃ§in tercih edildi.

â¸»

ğŸ“Š SonuÃ§lar

Model	CV RMSE
Ensemble (Ridge)	72300
Stacked NN	72468 (ancak LBâ€™de daha baÅŸarÄ±lÄ±)

	â€¢	LB skoru daha iyi olduÄŸu iÃ§in stacked NN birinciliÄŸi getirdi.
	â€¢	OOF tahminleri Ã¶zellik olarak kullanmak, baÅŸarÄ±yÄ± Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rdÄ±.

â¸»

ğŸ§© Ek Bilgiler ve Notlar
	â€¢	XGBoost, finalde kazara dahil edildi ancak katkÄ±sÄ± sÄ±nÄ±rlÄ±ydÄ±.
	â€¢	Chris Deotteâ€™un feature engineering fikirleri bu veri setinde iÅŸe yaramadÄ±.
	â€¢	AutoML Ã¼zerine yapÄ±lan tartÄ±ÅŸmalar (Ã¶zellikle AutoGluon) Ã§ok faydalÄ± oldu.


ğŸ“Œ Ã–neri NiteliÄŸinde KullanÄ±m

YukarÄ±daki yÃ¶ntemler:
	â€¢	KatmanlÄ± (stacked) modelleme
	â€¢	SÄ±zÄ±ntÄ±sÄ±z hedef kodlama
	â€¢	OOF tahminlerin yeniden kullanÄ±mÄ±
	â€¢	Outlier sÄ±nÄ±flandÄ±rmasÄ±

gibi teknikleri kullanarak benzer regresyon problemlerinde yÃ¼ksek performans elde edilebilir.
